# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qUlCC2TNeCKscpS1JZVaFaXmEaTz9JUo
"""

!pip install transformers

from transformers import pipeline
classifier = pipeline("sentiment-analysis")
classifier("I've been waiting for a HuggingFace course my whole life.")

classifier("i hate to hate and love to love and live with ppeace")

import pandas as pd
column_names = ['com_id', 'com_name', 'cus_target', 'cus_comment']
df = pd.read_csv('/content/twitter_training.csv', names=column_names, header=None)
df.head()

df.info()

column_names = ['com_name', 'cus_target', 'cus_comment']
for i in column_names:
  df[i] = df[i].astype(str)
  df[i] = df[i].str.lower()
df.dropna(inplace=True)

df['cus_comment2'] = df['cus_comment'].apply(classifier)

comparing = df.groupby('cus_name')[['cus_comment2', 'cus_target']].agg(lambda x: x.value_counts().index[0])

import torch
import torch.nn as nn
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

stop_words = set(stopwords.words('english'))

def processing(text):
    tokens = word_tokenize(text.lower())
    # Filter stops and non-alphabetic (punctuation)
    return [w for w in tokens if w not in stop_words and w.isalpha()]

# --- CRITICAL STEP: Create a Vocabulary ---
all_tokens = []
for comment in df['cus_comment']:
    all_tokens.extend(processing(comment))

vocab = {word: i+1 for i, word in enumerate(set(all_tokens))} # i+1 to save 0 for padding
vocab['<PAD>'] = 0
VOCAB_SIZE = len(vocab)

# Map target labels to numerical IDs
# First, we need  get all unique target labels
unique_targets = df['cus_target'].unique().tolist()
target_to_id = {label: i for i, label in enumerate(unique_targets)}
id_to_target = {i: label for label, i in target_to_id.items()}

# Transform the 'cus_target' column to numerical IDs
df['cus_target_id'] = df['cus_target'].map(target_to_id)

class NeuralNetwork(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        # 1. Embedding layer turns Word IDs into dense vectors (math space)
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # 2. LSTM processes the sequence
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        # 3. Linear layer maps hidden state to your classes
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        # text shape: [batch_size, sequence_length]
        embedded = self.embedding(text)
        output, (hidden, cell) = self.rnn(embedded)

        # We only care about the last hidden state (the summary of the sentence)
        last_hidden = hidden[-1]
        return self.fc(last_hidden)

# Initialize Model
# vocab_size, embed_dim, hidden_dim, output_dim (number of unique target classes)
model = NeuralNetwork(VOCAB_SIZE, 100, 256, len(unique_targets))
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(5):
    for i in range(len(df)):
        # 1. Convert text to tokens
        tokens = processing(df['cus_comment'][i])

        # 2. Convert tokens to IDs using our vocab dictionary
        ids = [vocab[w] for w in tokens if w in vocab]
        if not ids: continue # Skip empty processed sentences

        # 3. Prepare Tensors
        inputs = torch.tensor([ids], dtype=torch.long) # Shape [1, seq_len]
        # Use the numerical target ID
        label = torch.tensor([df['cus_target_id'][i]], dtype=torch.long)

        # 4. Standard PyTorch Training Step
        optimizer.zero_grad()
        outputs = model(inputs) # Forward pass
        loss = criterion(outputs, label)
        loss.backward()         # Backpropagation
        optimizer.step()        # Update weights

        if i % 100 == 0:
            model.eval() # Set model to evaluation mode (turns off Dropout)
            with torch.no_grad():
                # Get a test sentence
                test_tokens = processing(df['cus_comment'][i])
                test_ids = [vocab[w] for w in test_tokens if w in vocab]

                if test_ids:
                    test_inputs = torch.tensor([test_ids], dtype=torch.long)
                    test_outputs = model(test_inputs)

                    # Convert raw scores (logits) to a prediction (0 or 1)
                    prediction = torch.argmax(test_outputs, dim=1).item()
                    # Get the actual original string label for printing
                    actual_id = df['cus_target_id'][i]
                    actual_label = id_to_target[actual_id]
                    predicted_label = id_to_target[prediction]

                    print(f"--- Check at Batch {i} ---")
                    print(f"Comment: {df['cus_comment'][i][:50]}...")
                    print(f"Predict: {predicted_label} | Actual: {actual_label}")

            model.train() # Set model back to training mode
            print(f"Epoch {epoch} | Batch {i} | Loss: {loss.item():.4f}")

